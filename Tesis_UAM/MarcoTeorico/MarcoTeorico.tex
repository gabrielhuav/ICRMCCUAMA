%!TEX root = ../ICR.tex
\chapter{Marco Teórico \label{cap:MarcoTeorico}}

En esta sección se describen las bases teóricas y los conceptos fundamentales que sustentan las dos metodologías de detección de noticias falsas desarrolladas en este proyecto de investigación. Aunque las técnicas son aplicables a diferentes tipos de fraude digital, el enfoque específico se centra en la detección de desinformación periodística. Se abordan desde las técnicas clásicas de representación de texto y optimización metaheurística, hasta los paradigmas de aprendizaje profundo que definen el estado del arte actual.

\section{Detección de Noticias Falsas: Fundamentos y Extensibilidad}
\label{sec:deteccion_noticias_falsas}

La detección de noticias falsas constituye un problema multifacético que requiere un enfoque interdisciplinario, con metodologías que pueden extenderse a otros tipos de fraude digital. La desinformación se caracteriza por ser información deliberadamente falsa o engañosa que se presenta como noticia legítima \cite{bondielli2019survey}. Esta problemática ha evolucionado significativamente con el advenimiento de las redes sociales y los medios digitales, donde la velocidad de propagación supera ampliamente la capacidad de verificación tradicional.

En el contexto de la detección automatizada, se han explorado diferentes enfoques y técnicas. Das et al. \cite{das2022heuristic} propusieron un marco de ensamblaje basado en incertidumbre e impulsado por heurísticas para la detección de noticias falsas en tuits y artículos de noticias. Este enfoque parte de la premisa de que diferentes modelos pueden tener distintas fortalezas y debilidades, y que la combinación inteligente de múltiples predictores puede superar las limitaciones individuales.

El equipo de la Southern Methodist University presentó una solución pionera utilizando procesamiento de lenguaje natural y aprendizaje profundo para analizar tanto titulares como el contenido completo de las noticias \cite{thota2018fake}. Su trabajo estableció un precedente importante al demostrar la viabilidad de la vectorización TF-IDF combinada con redes neuronales densas.

Complementariamente, se ha propuesto un enfoque basado en análisis estilométrico utilizando Procesamiento del Lenguaje Natural (PLN) y Reconocimiento de Entidades Nombradas (NER) para identificar patrones lingüísticos que sugieran baja veracidad de la información \cite{tsai2023stylometric}. Este enfoque aprovecha la hipótesis de que los autores de contenido falso pueden exhibir patrones de escritura distintivos.

\subsection{Impacto Social y Desafíos de la Desinformación}

La rápida propagación de las noticias falsas a través de las redes sociales puede tener graves consecuencias en la sociedad. Como documentan Ali y Zain-Ul-Abdin \cite{ali2020posttruth}, la desinformación puede:

\begin{itemize}
    \item \textbf{Distorsionar la realidad:} Alterando la percepción pública de eventos y hechos
    \item \textbf{Manipular la opinión pública:} Influenciando procesos democráticos y decisiones sociales
    \item \textbf{Incitar a la violencia:} Promoviendo comportamientos agresivos basados en información errónea
    \item \textbf{Difundir propaganda política:} Siendo utilizada como herramienta de influencia partidista
    \item \textbf{Fomentar el odio:} Exacerbando divisiones sociales y promoviendo discriminación
    \item \textbf{Provocar pánico:} Especialmente en contextos de crisis sanitarias como la pandemia de COVID-19 \cite{perez2020fake}
\end{itemize}

\subsection{El Problema de las Heurísticas Cognitivas}

La investigación de Ali et al. \cite{ali2021fake} ha demostrado que las heurísticas cognitivas humanas, como la popularidad social (número de "me gusta" o compartidos), influyen significativamente en la percepción de credibilidad. Este fenómeno complica la detección, ya que el contenido falso puede volverse viral precisamente por aprovechar estos sesgos cognitivos.

\subsection{Diferenciación y Transferibilidad: Noticias Falsas vs. Fraude Digital}

Es fundamental establecer una distinción clara entre los conceptos centrales de este trabajo:

\begin{itemize}
    \item \textbf{Noticias Falsas (\glspl{noticiafalsa}):} Información deliberadamente engañosa que se presenta con formato periodístico legítimo, cuyo objetivo principal es manipular la opinión pública, influir en decisiones políticas o sociales, o crear confusión sobre eventos actuales. Se caracteriza por imitar el estilo de medios de comunicación establecidos.

    \item \textbf{Fraude Digital (\gls{fraudedigital}):} Conjunto más amplio de actividades maliciosas en entornos digitales orientadas principalmente a obtener beneficios económicos ilícitos, acceso no autorizado a información sensible, o perpetrar estafas. Incluye phishing, esquemas Ponzi digitales, fraude financiero, estafas de inversión, entre otros.
\end{itemize}

\textbf{Enfoque de esta investigación:} Este proyecto se centra específicamente en la detección de noticias falsas, utilizando técnicas de procesamiento de lenguaje natural y optimización metaheurística. Sin embargo, la metodología desarrollada es inherentemente transferible a otros tipos de fraude digital, ya que ambos problemas comparten características fundamentales:

\begin{itemize}
    \item \textbf{Naturaleza textual:} Ambos tipos de contenido requieren análisis de texto
    \item \textbf{Patrones lingüísticos:} Pueden exhibir características estilométricas distintivas
    \item \textbf{Clasificación binaria:} Pueden abordarse como problemas de clasificación legítimo/fraudulento
    \item \textbf{Optimización de parámetros:} Ambos se benefician de técnicas de calibración de hiperparámetros
\end{itemize}

\textbf{Adaptaciones necesarias para transferencia:} Para aplicar la metodología a otros tipos de fraude digital sería necesario: (1) construir corpus específicos del dominio objetivo, (2) ajustar características de preprocesamiento según el tipo de contenido, (3) re-calibrar hiperparámetros para el nuevo dominio, y (4) potencialmente incorporar características adicionales específicas del tipo de fraude.

\section{Representación de Texto: Desde BoW hasta Embeddings Contextuales}
\label{sec:representacion_texto}

La evolución de las técnicas de representación textual ha sido fundamental para el progreso en PLN. Esta sección aborda desde los métodos clásicos hasta las representaciones más sofisticadas utilizadas en esta tesis.

\subsection{Bolsa de Palabras (Bag-of-Words - BoW)}

El modelo de Bolsa de Palabras (BoW) constituye una de las técnicas más fundamentales para la representación de texto. A pesar de su simplicidad, sigue siendo relevante como línea base y componente en sistemas híbridos. El proceso implica:

\begin{enumerate}
    \item \textbf{Construcción del vocabulario:} Creación de un diccionario con todas las palabras únicas presentes en el corpus de documentos
    \item \textbf{Vectorización:} Representación de cada documento como un vector disperso, donde cada dimensión corresponde a una palabra del vocabulario y su valor representa la frecuencia de aparición
\end{enumerate}

Aunque este método ignora la gramática y el orden de las palabras, ha demostrado ser efectivo como base para métodos más sofisticados y fue fundamental en los primeros trabajos de clasificación de noticias en español \cite{acosta2019construccion}.

\subsection{Ponderación TF-IDF (Term Frequency-Inverse Document Frequency)}

La técnica TF-IDF mejora significativamente la representación BoW al introducir ponderación semántica. Se calcula como el producto de dos componentes:

\begin{equation}
\text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D)
\end{equation}

Donde:
\begin{itemize}
    \item \textbf{Frecuencia de Término (TF):} $\text{TF}(t,d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}$ - frecuencia relativa del término $t$ en el documento $d$
    \item \textbf{Frecuencia Inversa de Documento (IDF):} $\text{IDF}(t,D) = \log\frac{|D|}{|\{d \in D : t \in d\}|}$ - importancia global del término en la colección
\end{itemize}

Esta ponderación asigna mayor peso a términos que son frecuentes en un documento específico pero raros en el corpus general, mejorando la capacidad discriminativa del modelo \cite{thota2018fake}.

\subsection{Limitaciones de los Métodos Clásicos}

Los enfoques basados en BoW y TF-IDF presentan limitaciones fundamentales:
\begin{itemize}
    \item \textbf{Pérdida de información secuencial:} No capturan el orden ni la estructura sintáctica
    \item \textbf{Problema de dispersidad:} Generan representaciones muy dispersas en vocabularios grandes
    \item \textbf{Ausencia de semántica:} No modelan relaciones semánticas entre palabras
    \item \textbf{Falta de contexto:} Una palabra tiene la misma representación independientemente del contexto
\end{itemize}

\section{La Revolución Transformer y los Modelos de Lenguaje Modernos}
\label{sec:transformers_modelos_lenguaje}

\subsection{La Arquitectura Transformer: Fundamentos}

La arquitectura Transformer, introducida por Vaswani et al. \cite{vaswani2017attention} en el trabajo seminal "Attention Is All You Need", revolucionó el campo del PLN. Su innovación principal radica en el mecanismo de \textbf{auto-atención (self-attention)}, que permite al modelo calcular representaciones ponderando dinámicamente la importancia de cada elemento en una secuencia con respecto a todos los demás elementos.

\subsubsection{Mecanismo de Atención Multi-Cabeza}

El mecanismo de atención se define matemáticamente como:

\begin{equation}
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

Donde $Q$, $K$, y $V$ representan las matrices de consultas (queries), claves (keys) y valores (values), respectivamente. La atención multi-cabeza extiende este concepto:

\begin{equation}
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}

Esta arquitectura permite capturar diferentes tipos de relaciones lingüísticas simultáneamente, superando las limitaciones de los modelos secuenciales previos como RNNs y LSTMs.

\subsection{BERT y la Era del Pre-entrenamiento Bidireccional}

BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2018bert} introdujo el paradigma de pre-entrenamiento bidireccional, entrenando el modelo para predecir palabras enmascaradas considerando tanto el contexto izquierdo como el derecho. Este enfoque genera representaciones contextuales más ricas que los modelos unidireccionales previos.

\subsubsection{Variantes de BERT para Eficiencia}

El éxito de BERT motivó el desarrollo de variantes más eficientes:
\begin{itemize}
    \item \textbf{DistilBERT} \cite{sanh2019distilbert}: Utiliza destilación de conocimiento para reducir el tamaño del modelo en un 40\% manteniendo el 97\% del rendimiento
    \item \textbf{TinyBERT} \cite{jiao2019tinybert}: Aplica destilación a nivel de transformador y predicción para crear modelos aún más compactos
\end{itemize}

\subsection{Aplicaciones en Detección de Noticias Falsas en Español}

Para el español específicamente, se han desarrollado modelos especializados y evaluaciones:
\begin{itemize}
    \item Martínez-Gallego et al. \cite{martinez2021fake} exploraron la aplicación de BERT y BETO (BERT en español), estableciendo líneas base importantes
    \item Blanco-Fernández et al. \cite{blanco2024enhancing} compararon sistemáticamente BERT y RoBERTa para detección de desinformación política
    \item Shushkevich et al. \cite{shushkevich2023improving} investigaron la mejora de clasificación multiclase usando datos aumentados con ChatGPT
\end{itemize}

\subsection{Grandes Modelos de Lenguaje (LLMs) y Nuevos Paradigmas}

\subsubsection{GPT-3 y el Paradigma Few-Shot}

GPT-3 \cite{brown2020language} marcó un hito al demostrar capacidades emergentes de few-shot learning. Con 175 mil millones de parámetros, mostró que los modelos de gran escala pueden realizar tareas sin ajuste fino específico, solo con ejemplos en el prompt.

\subsubsection{LLaMA y la Democratización de LLMs}

LLaMA \cite{touvron2023llama} representa el esfuerzo por democratizar el acceso a modelos de gran escala, proporcionando alternativas open-source a modelos propietarios. Su arquitectura optimizada permite un rendimiento competitivo con menor costo computacional.

\subsubsection{Modelos Multimodales: Gemini}

Gemini \cite{gemini2023family} introduce capacidades multimodales nativas, procesando texto, imágenes y audio de forma integrada. Esta capacidad es relevante para la detección de desinformación que cada vez más incorpora elementos multimedia.

\subsubsection{IA Constitucional}

El trabajo de Bai et al. \cite{bai2022constitutional} sobre IA Constitucional aborda la alineación y seguridad de los LLMs, estableciendo principios para entrenar modelos más seguros y alineados con valores humanos. Este enfoque es crucial cuando los LLMs se utilizan para tareas de detección de desinformación.

\subsection{El Doble Rol de los LLMs en Detección}

La investigación reciente ha revelado que los LLMs presentan un doble rol:
\begin{itemize}
    \item \textbf{Como "Buenos Consejeros":} Pueden ser fine-tuned para detectar noticias falsas con alta precisión \cite{hu2024bad}
    \item \textbf{Como "Malos Actores":} Pueden generar desinformación convincente, complicando la detección \cite{su2023fake}
    \item \textbf{Adaptación Necesaria:} Los sistemas de detección deben adaptarse a la era de LLMs \cite{su2023adapting}
\end{itemize}

\section{Optimización Metaheurística en Detección de Fraude}
\label{sec:optimizacion_metaheuristica}

\subsection{Fundamentos de las Metaheurísticas}

Las metaheurísticas son estrategias de optimización de alto nivel que guían procesos de búsqueda para encontrar soluciones de alta calidad en espacios de búsqueda complejos. A diferencia de los algoritmos exactos, las metaheurísticas buscan soluciones "suficientemente buenas" en tiempo computacional razonable \cite{anselmo2013diseno}.

En el contexto de la detección de noticias falsas, las metaheurísticas abordan varios problemas de optimización, con metodologías transferibles a otros tipos de fraude digital:
\begin{itemize}
    \item \textbf{Calibración de hiperparámetros:} Optimización de parámetros de modelos de ML/DL
    \item \textbf{Selección de características:} Identificación de subconjuntos óptimos de variables
    \item \textbf{Arquitectura neural:} Diseño automático de topologías de red
    \item \textbf{Balanceado de datos:} Optimización de técnicas de muestreo
\end{itemize}

\subsection{Aplicaciones en Detección de Noticias Falsas}

\subsubsection{Modelado como Problema de Scheduling}

Aqil y Lahby \cite{aqil2021modeling} propusieron una perspectiva innovadora, modelando la detección de noticias falsas como un problema de Job Shop Scheduling. Compararon tres metaheurísticas:
\begin{itemize}
    \item \textbf{Algoritmo Genético (GA):} Evolución de soluciones mediante selección, cruce y mutación
    \item \textbf{Optimización por Enjambre de Partículas (PSO):} Búsqueda inspirada en comportamiento de bandadas
    \item \textbf{Colonia de Abejas Artificiales (ABC):} Algoritmo basado en comportamiento de forrajeo de abejas
\end{itemize}

Sus resultados mostraron que el algoritmo Iterated Greedy (IG) superó a las metaheurísticas bioinspiradas en este contexto específico.

\subsubsection{Optimización de Hiperparámetros en Deep Learning}

Bacanin et al. \cite{bacanin2023benefits} demostraron los beneficios de usar metaheurísticas para la calibración de hiperparámetros en modelos de deep learning. Su investigación mostró mejoras significativas sobre métodos tradicionales como grid search y random search:

\begin{itemize}
    \item \textbf{Eficiencia computacional:} Exploración más inteligente del espacio de hiperparámetros
    \item \textbf{Evitación de óptimos locales:} Capacidad de escape de configuraciones subóptimas
    \item \textbf{Adaptabilidad:} Ajuste dinámico según el comportamiento del modelo
\end{itemize}

La investigación que publiqué en \cite{hurtado2024calibracion} específicamente aborda la calibración de hiperparámetros en algoritmos metaheurísticos para detección de noticias falsas (como caso específico de fraude digital), proporcionando un marco metodológico relevante para esta tesis.

\subsubsection{Frameworks de Ensemble Heurístico}

Das et al. \cite{das2022heuristic} desarrollaron un marco de ensemble que combina:
\begin{itemize}
    \item \textbf{Modelos pre-entrenados:} BERT y similares para captura semántica
    \item \textbf{Características estadísticas:} Metadatos como URL, autor, timestamp
    \item \textbf{Heurísticas de incertidumbre:} Medidas de confianza para ponderación de predictores
\end{itemize}

Este enfoque híbrido logró F1-scores superiores al 95% en conjuntos de datos de referencia.

\subsection{Metaheurísticas para Detección de Fraude Financiero}

\subsubsection{Optimización Multi-objetivo}

Hidayattullah et al. \cite{hidayattullah2020financial} aplicaron metaheurísticas para optimizar la detección de fraude en estados financieros. Su enfoque multi-objetivo consideró:
\begin{itemize}
    \item \textbf{Precisión de clasificación:} Maximización de métricas de rendimiento
    \item \textbf{Eficiencia computacional:} Minimización de tiempo de entrenamiento
    \item \textbf{Robustez:} Estabilidad ante variaciones en los datos
\end{itemize}

Los resultados mostraron que SVM optimizado con Algoritmo Genético alcanzó 96.15\% de precisión, superando significativamente a métodos tradicionales.

\subsubsection{Optimización de Procesos Gaussianos}

Horak y Sabek \cite{horak2023gaussian} exploraron la optimización de hiperparámetros en Gaussian Process Regression para predicción de dificultades financieras. Su trabajo destaca la importancia de la calibración metaheurística en modelos probabilísticos.

\subsection{Algoritmos Híbridos Modernos}

\subsubsection{Enfoques Multi-thread}

Yildirim \cite{yildirim2023novel} propuso un enfoque metaheurístico híbrido multi-thread que optimiza simultáneamente:
\begin{itemize}
    \item Selección de características
    \item Parámetros del algoritmo de clasificación
    \item Arquitectura del ensemble
\end{itemize}

\subsubsection{PSO Adaptativo}

Deshai y Bhaskara Rao \cite{deshai2023unmasking} desarrollaron un enfoque CNN con PSO adaptativo para detectar reseñas falsas online. Su algoritmo PSO adaptativo ajusta dinámicamente los parámetros de velocidad e inercia basándose en el rendimiento de la iteración actual.

\section{Integración de Enfoques: Hacia Sistemas Híbridos}
\label{sec:integracion_enfoques}

\subsection{Combinación de Representaciones Clásicas y Modernas}

La tendencia actual en detección de noticias falsas favorece sistemas híbridos que combinan (principios aplicables a otros tipos de fraude digital):
\begin{itemize}
    \item \textbf{Características lingüísticas tradicionales:} TF-IDF, n-gramas, métricas de legibilidad
    \item \textbf{Embeddings contextuales:} Representaciones de BERT y similares
    \item \textbf{Metadatos estructurales:} Información temporal, de red social, y fuente
    \item \textbf{Características estilométricas:} Patrones de puntuación, longitud de oraciones, complejidad sintáctica
\end{itemize}

\subsection{Arquitecturas de Ensemble Optimizadas}

Las arquitecturas modernas emplean:
\begin{itemize}
    \item \textbf{Ensemble de modelos heterogéneos:} Combinación de clasificadores tradicionales y deep learning
    \item \textbf{Ponderación adaptativa:} Pesos dinámicos basados en confianza y rendimiento
    \item \textbf{Fusión de decisiones:} Estrategias sofisticadas para combinar predicciones múltiples
\end{itemize}

\subsection{Optimización End-to-End}

Zhang et al. \cite{zhang2023using} exploraron el uso de LLMs para optimización de hiperparámetros, abriendo nuevas posibilidades para la optimización automática de pipelines completos de detección.

\section{Desafíos y Direcciones Futuras}
\label{sec:desafios_direcciones}

\subsection{Desafíos Técnicos}

\begin{itemize}
    \item \textbf{Adaptación multilingüe:} Desarrollo de modelos robustos para múltiples idiomas
    \item \textbf{Detección en tiempo real:} Optimización para procesamiento de streams de alta velocidad
    \item \textbf{Robustez adversarial:} Resistencia a ataques de evasión sofisticados
    \item \textbf{Explicabilidad:} Desarrollo de modelos interpretables para decisiones críticas
\end{itemize}

\subsection{Consideraciones Éticas y Sociales}

\begin{itemize}
    \item \textbf{Sesgos algorítmicos:} Mitigación de discriminación en sistemas de detección
    \item \textbf{Privacidad:} Protección de datos personales en análisis de contenido
    \item \textbf{Transparencia:} Apertura en metodologías y criterios de detección
    \item \textbf{Impacto social:} Consideración de efectos en libertad de expresión y democracia
\end{itemize}

\section{Síntesis del Marco Teórico}
\label{sec:sintesis_marco}

Este marco teórico establece los fundamentos para las dos metodologías desarrolladas en esta tesis:

\begin{enumerate}
    \item \textbf{Enfoque Clásico Optimizado:} Utiliza representaciones TF-IDF con optimización metaheurística de hiperparámetros, proporcionando una línea base sólida y computacionalmente eficiente
    
    \item \textbf{Enfoque de Deep Learning:} Emplea modelos Transformer pre-entrenados con fine-tuning, aprovechando representaciones contextuales sofisticadas
\end{enumerate}

La combinación de ambos enfoques, sustentada en la literatura revisada, permite abordar el problema de detección de noticias falsas desde múltiples perspectivas, maximizando tanto la efectividad como la eficiencia computacional del sistema propuesto. La metodología desarrollada establece principios transferibles para abordar otros tipos de fraude digital con las adaptaciones correspondientes.

El marco establece también la base teórica para la contribución metodológica principal de esta tesis: la aplicación sistemática de técnicas metaheurísticas para la optimización de hiperparámetros en ambos paradigmas, desde clasificadores tradicionales hasta modelos de deep learning, en el contexto específico de textos en español.